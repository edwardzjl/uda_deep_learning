{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified ./notMNIST_large.tar.gz\n",
      "Found and verified ./notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '.' # Change me to store data elsewhere\n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "    \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n",
    "    slow internet connections. Reports every 5% change in download progress.\n",
    "    \"\"\"\n",
    "    global last_percent_reported\n",
    "    percent = int(count * blockSize * 100 / totalSize)\n",
    "    \n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 == 0:\n",
    "            sys.stdout.write(\"%s%%\" % percent)\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        last_percent_reported = percent\n",
    "\n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    dest_filename = os.path.join(data_root, filename)\n",
    "    if force or not os.path.exists(dest_filename):\n",
    "        print('Attempting to download:', filename)\n",
    "        filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "        print('\\nDownload Complete!')\n",
    "    statinfo = os.stat(dest_filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', dest_filename)\n",
    "    else:\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "    return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./notMNIST_large already present - Skipping extraction of ./notMNIST_large.tar.gz.\n",
      "['./notMNIST_large/A', './notMNIST_large/B', './notMNIST_large/C', './notMNIST_large/D', './notMNIST_large/E', './notMNIST_large/F', './notMNIST_large/G', './notMNIST_large/H', './notMNIST_large/I', './notMNIST_large/J']\n",
      "./notMNIST_small already present - Skipping extraction of ./notMNIST_small.tar.gz.\n",
      "['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D', './notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', './notMNIST_small/I', './notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "    if os.path.isdir(root) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall(data_root)\n",
    "        tar.close()\n",
    "    data_folders = [\n",
    "        os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "        if os.path.isdir(os.path.join(root, d))]\n",
    "    if len(data_folders) != num_classes:\n",
    "        raise Exception(\n",
    "            'Expected %d folders, one per class. Found %d instead.' % (\n",
    "                num_classes, len(data_folders)))\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "\n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACRUlEQVR4nG2STUiUYRDH//O8z368\nvvu9qGyZH7upaaYb9GEYlVFRknTJsFsWHTt1rEvnunSIQKhDF6EuBR6MwkzE0CQJKiFF11owd9Vd\nM0133/eZDtruYs1pmP/8/swwA+TCZRB0GyhfKUi9zpbwSNn7KWJsD4Ezj4d4+tvtCvEfreH+GGcs\njt/Nu+XbxO9qlmB/Sd0/qA0XlVLMnJ6/k5tkq0tkz3fNKwLYa2gV22wJh9itWRYYc5fLcsiW6HLW\nGUzmIni35yC4UCQzsj+saO3N+DipYNVplV+f4K58Ns4WD/uv9/1kjt2r3YQ2yeLETNQkNbA8s6wB\npd7yvC1rqQtXVyRNJAID/SMxOE85vIpypLzkM3hpcHhBvf5RzDBaG3IkWc3NmkBqJQ271S9ZFe+I\nh7S/trvML0BqcvirWF9K9whYnS1rNgASIOG5doRJD54ITB2eMU4u+bTMlbFpABLE/r1NToKokY3R\nwY6AYSfQgcjiIjGgaU3dSVaKmVmtWhvMzCY/DUtAgOjY2Q0wccYC67BDKQD1uhMQZLoDJT4I0yRO\nILH6fZ0FC+j1RSDJjrZmMjj7cK79ecyfOqr37uuotVxVNz4kCLL+5ifO8pNzERR54LOHg/Zbr2bZ\n+tgDErqjuoZlKrYQx/qKXM5OpzIPRn0QO5NdmnA3ttpUenB2coMUm8zEWvrlrwX2tFWawt8ZRppG\ne9XWhZkVJoYcq7aIuUfi7bvS4y8eJQu/zdeth/qin+OQoUqbSycqFPWgLDec7e4/wqfzL3yRo74A\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='notMNIST_small/A/MDEtMDEtMDAudHRm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./notMNIST_large/A.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/B.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/C.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/D.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/E.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/F.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/G.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/H.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/I.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/J.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/A.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/B.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/C.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/D.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/E.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/F.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/G.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/H.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/I.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/J.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    \"\"\"Load the data for a single letter label.\"\"\"\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "        try:\n",
    "            image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "            dataset[num_images, :, :] = image_data\n",
    "            num_images = num_images + 1\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('Mean:', np.mean(dataset))\n",
    "    print('Standard deviation:', np.std(dataset))\n",
    "    return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            # You may override by setting force=True.\n",
    "            print('%s already present - Skipping pickling.' % set_filename)\n",
    "        else:\n",
    "            print('Pickling %s.' % set_filename)\n",
    "            dataset = load_letter(folder, min_num_images_per_class)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':', e)\n",
    "    return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5        -0.5        -0.5        -0.5        -0.5        -0.24901961\n",
      "  0.05686275  0.2764706   0.46862745  0.48823529  0.5         0.5\n",
      "  0.49215686  0.47254902  0.46078432  0.46078432  0.46078432  0.46078432\n",
      "  0.47647059  0.5         0.5         0.49607843  0.3509804   0.20196079\n",
      " -0.02941176 -0.22941177 -0.40980393 -0.5       ]\n"
     ]
    }
   ],
   "source": [
    "aaa = np.load(open(\"./notMNIST_small/A.pickle\", \"rb\"))\n",
    "print(aaa[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFTFJREFUeJzt3XtwlFWaBvDnzYUAAZQAYrgZUESRcVAjgsOOzrgiMihq\nuQjjCioaS8Vdb6ssM7XjzDoW5ais1oqKI4qWi04p3lYUlNEFR0WCFwQRuQgKJtwdQCSX7nf/SDOV\n0Zz3NPk6/XU8z6+KStJPn/RJhzd9eb/vHFFVEFF48uKeABHFg8VPFCgWP1GgWPxEgWLxEwWKxU8U\nKBY/UaBY/ESBYvETBaogmzfWRoq0LYqzeZM/CMnO9n1WcFiNM+vV5mtzbD3EzD/fdZiZF236xswp\nu/bjG9Rqjf1LTYlU/CIyEsC9APIB/FFVp1nXb4tinCJnRLnJ3CSe+zriIdR7Rww18+6T1zmzaUc8\nb47dmWhr5hc/f62ZH3XzUjOHJo2Mh5Zn2hJdmPZ1m/20X0TyAdwP4GwAAwGMF5GBzf1+RJRdUV7z\nDwGwVlXXq2otgKcAjMnMtIiopUUp/p4Avmz09abUZX9HRCpEpFJEKuvgfm1KRNnV4u/2q+pMVS1X\n1fJCFLX0zRFRmqIU/2YAvRt93St1GRG1AlGKfymA/iLSV0TaABgH4MXMTIuIWlqzW32qWi8ikwHM\nR0Orb5aqrszYzHKNr51nDS2w7+bVDw428zEnVJp5Ut1z61tgt/KOLsw383XjHjTzgTuuMfPev3/b\nmfnuF62vN3OKJlKfX1XnAZiXobkQURbx8F6iQLH4iQLF4icKFIufKFAsfqJAsfiJApXV8/lzmq+P\nL8bfSeu0VQA6/3Az715jn3P/2c/am3li925nNrRisjn2//5jupm3kzZmfvUvXzbzV2b0c2aJr+2f\nu6VPlQ4dH/mJAsXiJwoUi58oUCx+okCx+IkCxeInChRbfQdYrTwASCac0WcPn2wOXTngfjO/8IRf\nmLnVygMAKXS347rOfMccO3b8BWY+b4B90uZFnT6xx/c51R3u2mWORZ59ujHU/TshPz7yEwWKxU8U\nKBY/UaBY/ESBYvETBYrFTxQoFj9RoMLp8/t6xkYfHwDqRpQ7s89/8bA59qxVdi8d2zaZsW+Ja+QZ\np756fu5V63rY33uA56btGJrHx5dcxd8MUaBY/ESBYvETBYrFTxQoFj9RoFj8RIFi8RMFKlKfX0Q2\nANgDIAGgXlXdzfCYidULh3f1bXx1RY0zS3gGf7Gzs5n3kc32jed7jlFIGMcoeI5faNOx1szrPOfM\nP7PnaDPP+6LKmcV6Nr7n+Afx3ee+5dpbwfbimTjI52equj0D34eIsohP+4kCFbX4FcACEVkmIhWZ\nmBARZUfUp/3DVXWziBwG4DUR+VRVFzW+QuqPQgUAtIW97RQRZU+kR35V3Zz6uBXAcwCGNHGdmapa\nrqrlhSiKcnNElEHNLn4RKRaRjgc+BzACwIpMTYyIWlaUp/3dATwnDTupFgD4H1V9NSOzIqIW1+zi\nV9X1AH6cwblE49nO2dd3zT/0EDO/8UcL3WM9a/53bL/fzKNuNa1J9/j8LiXm2FuOX2DmhWL3u+9+\n6Vwz77fDvW+Ab52CyL1y6/+E5/gH9eQ/BGz1EQWKxU8UKBY/UaBY/ESBYvETBYrFTxSocJbu9pDO\nh5r5OcWfGWkHc+zRnbeZ+Tbf6aW+Nqa1ffgUe+3tSYf82cxHrR5l5kf99iMzTxo/m1qnIqfDc79Y\nLdSvJwwzh3a+9Asz3/jmEWbe5z/trdHNLeGz1GbkIz9RoFj8RIFi8RMFisVPFCgWP1GgWPxEgWLx\nEwWKff40Ffp6yoarD7d76XeUjjbz+s1fmXn19ac6s7UXzzDH/vi98Wbe87JqM0/u22fmZi/ecypz\n1FN+d0109/IX33GfObZICs0cx9jxyZuuNvOSWTGe6pzCR36iQLH4iQLF4icKFIufKFAsfqJAsfiJ\nAsXiJwoU+/wH7HdvwQ0A6+vbOLOunt2cf9LW/hv71fllZr6nrI+Zz73wHmf2o3tuNMf2uOttM/ee\nWR7hnHovz5LoPtuGumfv6+NX1e8186757cy8zdgtZo5ZdpwNfOQnChSLnyhQLH6iQLH4iQLF4icK\nFIufKFAsfqJAifrOqRaZBWA0gK2qOih1WQmApwGUAdgAYKyq7vLdWCcp0VPkjIhTdk400vC8oiIz\n7/aGu8//aJ83zbH1nm75pnr7GIOxyy838+63uH+HiVVrzLHw7BkATXryaNuLmyIeQ1BQergzu+TN\nJebYcR29/51NEzb+1My3DNvtzKKcz79EF2K37kyrGNJ55H8MwMjvXDYFwEJV7Q9gYeprImpFvMWv\nqosA7PzOxWMAzE59PhvAeRmeFxG1sOa+5u+uqlWpz6sBdM/QfIgoSyK/4acNbxo4X3yJSIWIVIpI\nZR3s17ZElD3NLf4tIlIKAKmPW11XVNWZqlququWFsN9UI6LsaW7xvwhgYurziQBeyMx0iChbvMUv\nInMAvANggIhsEpFJAKYBOFNE1gD4x9TXRNSKeM/nV1XXwu4t1LBvJt+5355+dXL/fjOv2tfNmeV7\nbjvf8zf207qOZt51zDozTxj7uYvn+AWtyeH3YSKu619f5d5zYOq755tjx51pn3Cf8Px/+qC6l5n3\nwCdmng08wo8oUCx+okCx+IkCxeInChSLnyhQLH6iQLWqpbut1k7UbYvXPHaSmU/p8YozW15rtwmP\nK3SfDgwAI9vZ21z/+w2nmHnp3cby2wnv4tutl6+9a5wSnF8d7WhTX3tX3z000vfPBj7yEwWKxU8U\nKBY/UaBY/ESBYvETBYrFTxQoFj9RoHKrz+9ZRtrq5ecf298ce9QTG8x8kHxg5i+MONGZ3Xn9uebY\nteMfNHOfqRVzzPyJJ4c4s/qt2+1v3pJbbMfNmHv9IdGOf1hWU2vmfV6y73fr1jWZnfucj/xEgWLx\nEwWKxU8UKBY/UaBY/ESBYvETBYrFTxSo7Pf5rV6+sQQ1AGDo8c5o0uPPm0MTsPvZj53i7uMDQGLX\nJmc24L/MoXh1jH3u+JntvjVz33bRU+/s6cz6T9hijo2yHXTctL6u2WM79/hrpNu+9MNLzbzHJ/bS\n3C25NkW6+MhPFCgWP1GgWPxEgWLxEwWKxU8UKBY/UaBY/ESB8vb5RWQWgNEAtqrqoNRltwG4EsC2\n1NWmquq8tG7R6OXnDzjKHFrxxFxndl7xXnPszydMMvPCXcvMPK9tW2dW/6X7GAAAuGmWfdsrJ88w\n8xq1+9lrzvijM/uHcdeYYzs+9a6Zx3ocQMS1BvK7ubdV/8Nxz5hjffd5ySMdzLw1SOeR/zEAI5u4\nfLqqDk79S6/wiShneItfVRcB2JmFuRBRFkV5zT9ZRJaLyCwR6ZyxGRFRVjS3+B8AcCSAwQCqANzt\nuqKIVIhIpYhU1qGmmTdHRJnWrOJX1S2qmlDVJICHAThXkFTVmaparqrlhYi2OSIRZU6zil9ESht9\neT6AFZmZDhFlSzqtvjkATgfQVUQ2AfgNgNNFZDAABbABwFUtOEciagGiWVyXvVNeiQ4tOMt9hQXd\nzfGvHvOyMxu1epQ5NvHzKjOXfM+eAdY+95692qXQ/ht7xGJ7/EO93jHzOnXPbeG37c2x951j7zmQ\nWLXGzH17LXjXaDBEPcZgxxXDnFnl7x4wxx77l0vMvM8/fWzmcR0fsUQXYrfu9Bwg0YBH+BEFisVP\nFCgWP1GgWPxEgWLxEwWKxU8UqKwu3Z0oKcaOMSc786XH2O2XhCad2Rfzy8yxPfUrM/eyWqK+M09r\n7MOa191qLxu+9fHXzbyDFDqzke3t23760Wozrz7dfSozACRrPctn+07LjcDXTht+9VJntrLWXi69\n3617zLze83Nla5vtKPjITxQoFj9RoFj8RIFi8RMFisVPFCgWP1GgWPxEgcpqnz+/Sy26XPJF88cb\np84eurb5p45G5jlt1dePzn/jfTP/yZybzXzNP7uPj9iV2GeOfbTPYjPv99DlZt5/oj1382f3nUbt\nOT7iq1tONfNXS91Log+67yZzbM/1b5t5a97a/AA+8hMFisVPFCgWP1GgWPxEgWLxEwWKxU8UKBY/\nUaCy2ufvWfQ1ft/3OWeeUE8/3LNEdq4yl/0GvMtf95vynpmPHnK2M/vfo18xx/qOA1h/5iwz7z/t\najPvN8VYdtzTC08OH2zmb1z3BzM/fcXFzqzntIh9fN/vtBVondVERJGx+IkCxeInChSLnyhQLH6i\nQLH4iQLF4icKlLfPLyK9ATwOoDsABTBTVe8VkRIATwMoA7ABwFhV3WV9r3YCDGrjXu+8HnbvNN/4\nW7Wnt90rL27B9eO9fNug+6Zm7FcAAMmrip3Zgpfda/oDwIj29hbee5P7zXzNBHuvhYF7rnFmZXO3\nmWNveGyOmf/52x5mXny5e0+ByOvuZ3Fr+5aSziN/PYCbVHUggKEArhWRgQCmAFioqv0BLEx9TUSt\nhLf4VbVKVd9Pfb4HwCoAPQGMATA7dbXZAM5rqUkSUeYd1Gt+ESkDcAKAJQC6q2pVKqpGw8sCImol\n0i5+EekA4FkA16vq7saZqioa3g9oalyFiFSKSOX2Ha3/eGiiH4q0il9ECtFQ+E+q6tzUxVtEpDSV\nlwLY2tRYVZ2pquWqWt61i/2mHBFlj7f4RUQAPAJglare0yh6EcDE1OcTAbyQ+ekRUUsR9bQsRGQ4\ngMUAPgZwoOc0FQ2v+/8EoA+AjWho9e20vteg49voMy93deZHFrQz52Kd0nti5UXm2G7nrjZzKWxj\n5lpXa+YtKcoy0ftHDzHHPjnjHjPvVdDBzOvUfimXhLtNWVljPxP8Jllk5ndN+KWZy9sfuUPPadS+\n5dhz1RJdiN26M62+trfPr6pvwd2JPuNgJkZEuYNH+BEFisVPFCgWP1GgWPxEgWLxEwWKxU8UKG+f\nP5N6HHeoXvnUac78111XmOOTTR9BDACoSnxrjr3iomvNXN4xesIApMjdc9ZazzEAUe9jz+mn0sZ9\njIJvm+udlw0z86dus5fHLs23j4+wtM+zx/adP8nMj75smZlbx254l96Os8/vO/3cON5lSWJB2n1+\nPvITBYrFTxQoFj9RoFj8RIFi8RMFisVPFCgWP1GgstrnLyrrpYf/6l+c+efnPGyO/2vS3cs/JM9e\nC+D27ceY+dsXHmfmic/WmbkpQt8WQKw95+JF3cx87lGvmfm+ZPPXQahRewvvobNvMvOyXxvbg3v4\n1lBoSdb6DD4Hcz4/H/mJAsXiJwoUi58oUCx+okCx+IkCxeInChSLnyhQWe3zdyruoUMHVjjz++c+\nZI4vK3BvJ+3rCfvOHX9yTxczv/NB974APec1uVnR3yRWrzVznzzPNtp63JHOrHpYR3Ps6MsXm/nt\nh31s5ndsH2DmU7va+yVYfHsCFIq99v5Jy8Y6s26/s/cE0KX2z92SCkoPN/Pqc/s6s9XPTse+rV+y\nz09Ebix+okCx+IkCxeInChSLnyhQLH6iQLH4iQLl7fOLSG8AjwPoDkABzFTVe0XkNgBXAtiWuupU\nVZ1nfa9OUqKniHtX701TTzXnsnLyDDO3WGsBAP71ACyL9tv5W3vtXniN2ueOdy3Ya+bnd1zpzHoV\ndDDH7k3akx/y0I1mXvbfq8z8yyuOdWYLrrvTHFvqmXuN1pl5kRQ6s1W1+8yxFy+/zMyTr9vHhXTa\naB93smOg+3d+2gXvm2OXzjjBmX36wnR8sz29Pn86KxbUA7hJVd8XkY4AlonIgRUcpqvqXencEBHl\nFm/xq2oVgKrU53tEZBWAni09MSJqWQf1ml9EygCcAGBJ6qLJIrJcRGaJSGfHmAoRqRSRyjrYW0cR\nUfakXfwi0gHAswCuV9XdAB4AcCSAwWh4ZnB3U+NUdaaqlqtqeSHs46mJKHvSKn4RKURD4T+pqnMB\nQFW3qGpCVZMAHgYwpOWmSUSZ5i1+EREAjwBYpar3NLq8tNHVzgdgb7FLRDklnVbfcACLAXwMIJm6\neCqA8Wh4yq8ANgC4KvXmoFOnvBIdWnCWM/ctWbzxt+5W4MuX2m2jIwvttpFPQpPOLN+39HYLW1fn\nbgWOWHydObb/3fbS2vqBu40YVeL0E8285PaNZv6nfgszOZ2s+tz4nY397b+ZY0tmuZckP5ilu9N5\nt/8tAE19M7OnT0S5jUf4EQWKxU8UKBY/UaBY/ESBYvETBYrFTxSo7C7d7TmlF3n2UszWVtXJ09yn\nOQLAusvt1ucN5XbP+ELjtFnfBtqv7+tn5s9Un2TmaxaXmXnZS8Ypv+9FW4Lat1W1JuyfXvLdv9Mo\nW1EDwParhpl58QXVzuzmfvPNsScX2cuxv7avzMx/85fzzPyY6e7fWXLFp+ZY63fybv187E5yi24i\nMrD4iQLF4icKFIufKFAsfqJAsfiJAsXiJwpUVvv8IrINQOOTtLsC2J61CRycXJ1brs4L4NyaK5Nz\nO0JVu6VzxawW//duXKRSVctjm4AhV+eWq/MCOLfmimtufNpPFCgWP1Gg4i7+mTHfviVX55ar8wI4\nt+aKZW6xvuYnovjE/chPRDGJpfhFZKSIrBaRtSIyJY45uIjIBhH5WEQ+FJHKmOcyS0S2isiKRpeV\niMhrIrIm9bHJbdJimtttIrI5dd99KCKjYppbbxF5Q0Q+EZGVIvKvqctjve+MecVyv2X9ab+I5AP4\nDMCZADYBWApgvKp+ktWJOIjIBgDlqhp7T1hEfgpgL4DHVXVQ6rI7AexU1WmpP5ydVfXWHJnbbQD2\nxr1zc2pDmdLGO0sDOA/ApYjxvjPmNRYx3G9xPPIPAbBWVderai2ApwCMiWEeOU9VFwHY+Z2LxwCY\nnfp8Nhr+82SdY245QVWrVPX91Od7ABzYWTrW+86YVyziKP6eAL5s9PUm5NaW3wpggYgsE5GKuCfT\nhO6NdkaqBtA9zsk0wbtzczZ9Z2fpnLnvmrPjdabxDb/vG66qJwI4G8C1qae3OUkbXrPlUrsmrZ2b\ns6WJnaX/Js77rrk7XmdaHMW/GUDvRl/3Sl2WE1R1c+rjVgDPIfd2H95yYJPU1Ed7sbksyqWdm5va\nWRo5cN/l0o7XcRT/UgD9RaSviLQBMA7AizHM43tEpDj1RgxEpBjACOTe7sMvApiY+nwigBdinMvf\nyZWdm107SyPm+y7ndrxW1az/AzAKDe/4rwPwqzjm4JhXPwAfpf6tjHtuAOag4WlgHRreG5kEoAuA\nhQDWAHgdQEkOze0JNOzmvBwNhVYa09yGo+Ep/XIAH6b+jYr7vjPmFcv9xiP8iALFN/yIAsXiJwoU\ni58oUCx+okCx+IkCxeInChSLnyhQLH6iQP0/G7m65PDVm7AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6b70576080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a pic\n",
    "\n",
    "pickle_file = train_datasets[0]  # index 0 should be all As, 1 = all Bs, etc.\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    letter_set = pickle.load(f)  # unpickle\n",
    "    sample_idx = np.random.randint(len(letter_set))  # pick a random image index\n",
    "    sample_image = letter_set[sample_idx, :, :]  # extract a 2D slice\n",
    "    plt.figure()\n",
    "    plt.imshow(sample_image)  # display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "        labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "    num_classes = len(pickle_files)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "    train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "    vsize_per_class = valid_size // num_classes\n",
    "    tsize_per_class = train_size // num_classes\n",
    "\n",
    "    start_v, start_t = 0, 0\n",
    "    end_v, end_t = vsize_per_class, tsize_per_class\n",
    "    end_l = vsize_per_class + tsize_per_class\n",
    "    for label, pickle_file in enumerate(pickle_files):       \n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                letter_set = pickle.load(f)\n",
    "                # let's shuffle the letters to have random validation and training set\n",
    "                np.random.shuffle(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "                    valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "                    valid_labels[start_v:end_v] = label\n",
    "                    start_v += vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "                    \n",
    "                train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "                train_dataset[start_t:end_t, :, :] = train_letter\n",
    "                train_labels[start_t:end_t] = label\n",
    "                start_t += tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_file, ':', e)\n",
    "            raise\n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "\n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save data\n",
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'train_labels': train_labels,\n",
    "        'valid_dataset': valid_dataset,\n",
    "        'valid_labels': valid_labels,\n",
    "        'test_dataset': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 690800512\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 28, 28)\n",
      "(30000, 784)\n",
      "[8 3 8 ..., 1 9 9]\n"
     ]
    }
   ],
   "source": [
    "nsamples = 30000\n",
    "sampled_train_dataset = train_dataset[:nsamples, :, :]\n",
    "print(sampled_train_dataset.shape)\n",
    "\n",
    "_, nx, ny = sampled_train_dataset.shape\n",
    "sampled_train_dataset_2d = sampled_train_dataset.reshape((nsamples, nx * ny))\n",
    "print(sampled_train_dataset_2d.shape)\n",
    "\n",
    "sampled_train_labels = train_labels[:nsamples]\n",
    "print(sampled_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create linear regression object\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "logistic.fit(sampled_train_dataset_2d, sampled_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "nsamples, nx, ny = test_dataset.shape\n",
    "test_dataset_2d = test_dataset.reshape((nsamples, nx * ny))\n",
    "print(test_dataset_2d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = logistic.predict(test_dataset_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  11   12   14 ..., 9990 9995 9999]\n"
     ]
    }
   ],
   "source": [
    "non_zeros = np.nonzero(predicted_labels - test_labels)[0]\n",
    "print(non_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1170"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
